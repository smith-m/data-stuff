{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, List, Union, Any\n",
    "import os\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "API = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ndjson(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "conditions = read_ndjson('data/Condition.ndjson')\n",
    "patients = read_ndjson('data/Patient.ndjson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions.to_json('data/Condition.json')\n",
    "patients.to_json('data/Patient.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db setup\n",
    "# \n",
    "# create source tables\n",
    "# create etl tables\n",
    "# create destination tables - patient\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What files do we want to generate?\n",
    "#1 - Patient - schema\n",
    "# sources\n",
    "# - patients\n",
    "# - edstays\n",
    "# - admissions\n",
    "# - uuid_namespace\n",
    "# - map_gender\n",
    "# - map_marital_status\n",
    "# - fn_patient_extension\n",
    "#2 - Condition\n",
    "\n",
    "import json\n",
    "from genson import SchemaBuilder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st # import the Streamlit library\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain # import LangChain libraries\n",
    "from langchain.llms import OpenAI # import OpenAI model\n",
    "from langchain.prompts import PromptTemplate # import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 14:27:47.847 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/smithm/Library/Caches/pypoetry/virtualenvs/hackaithon-P5VjHMQO-py3.11/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the title of the Streamlit app\n",
    "st.title(\"âœ… What's TRUE  : Using LangChain `SimpleSequentialChain`\")\n",
    "\n",
    "# Add a link to the Github repository that inspired this app\n",
    "st.markdown(\"Inspired from [fact-checker](https://github.com/jagilley/fact-checker) by Jagiley\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# If an API key has been provided, create an OpenAI language model instance\n",
    "if API:\n",
    "    llm = OpenAI(temperature=0.7, openai_api_key=API)\n",
    "else:\n",
    "    # If an API key hasn't been provided, display a warning message\n",
    "    st.warning(\"Enter your OPENAI API-KEY. Get your OpenAI API key from [here](https://platform.openai.com/account/api-keys).\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 12:57:45.466 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/smithm/Library/Caches/pypoetry/virtualenvs/hackaithon-P5VjHMQO-py3.11/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "user_question = st.text_input(\n",
    "    \"Enter Your Question : \",\n",
    "    placeholder = \"Cyanobacteria can perform photosynthetsis , are they considered as plants?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.document_loaders import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: db/\n"
     ]
    }
   ],
   "source": [
    "loader = CSVLoader('data/mtsamples.csv')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=API)\n",
    "vectordb = Chroma.from_documents(texts, embeddings, persist_directory=\"db/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smithm/Library/Caches/pypoetry/virtualenvs/hackaithon-P5VjHMQO-py3.11/lib/python3.11/site-packages/langchain/chains/retrieval_qa/base.py:201: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(openai_api_key=API), chain_type='stuff', \n",
    "                                vectorstore=vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 3338 and 4758'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Which patient ids were diagnosed with heart disease?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 73\u001b[0m\n\u001b[1;32m     68\u001b[0m dest_sample \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/Patient.ndjson\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     69\u001b[0m source_files\u001b[39m=\u001b[39m[\n\u001b[1;32m     70\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdata/mimic-iv-clinical-database-demo-2.2/hosp/admissions.csv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     71\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdata/mimic-iv-clinical-database-demo-2.2/hosp/patients.csv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     72\u001b[0m ]\n\u001b[0;32m---> 73\u001b[0m plan \u001b[39m=\u001b[39m data_mapping_plan(\n\u001b[1;32m     74\u001b[0m     files\u001b[39m=\u001b[39;49msource_files,\n\u001b[1;32m     75\u001b[0m     destination_sample_file\u001b[39m=\u001b[39;49mdest_sample\n\u001b[1;32m     76\u001b[0m )\n",
      "Cell \u001b[0;32mIn[54], line 46\u001b[0m, in \u001b[0;36mdata_mapping_plan\u001b[0;34m(files, destination_sample_file)\u001b[0m\n\u001b[1;32m     43\u001b[0m destination_sample_prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m{\u001b[39;00mdestination_table\u001b[39m}\u001b[39;00m\u001b[39m sample:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(df_normalized\u001b[39m.\u001b[39mhead(\u001b[39m5\u001b[39m))\u001b[39m}\u001b[39;00m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     45\u001b[0m source_prompts \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 46\u001b[0m \u001b[39mfor\u001b[39;00m source_name, df \u001b[39min\u001b[39;00m loaded_source_files:\n\u001b[1;32m     47\u001b[0m     source_prompts\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m{\u001b[39;00msource_name\u001b[39m}\u001b[39;00m\u001b[39m sample:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(df\u001b[39m.\u001b[39mhead(\u001b[39m5\u001b[39m))\u001b[39m}\u001b[39;00m\u001b[39m\"\"\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m context_prompt \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([command, destination_prompt, destination_sample_prompt, \u001b[39m*\u001b[39msource_prompts])\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def flatten_nested_dictionary(dictionary, parent_key='', flattened_dict=None):\n",
    "    if flattened_dict is None:\n",
    "        flattened_dict = {}\n",
    "\n",
    "    for key, value in dictionary.items():\n",
    "        new_key = f\"{parent_key}.{key}\" if parent_key else key\n",
    "\n",
    "        if isinstance(value, dict):\n",
    "            flatten_nested_dictionary(value, new_key, flattened_dict)\n",
    "        else:\n",
    "            flattened_dict[new_key] = value\n",
    "\n",
    "    return flattened_dict\n",
    "\n",
    "def generate_schema(ndjson_file='data/Patient.ndjson'):\n",
    "    builder = SchemaBuilder()\n",
    "\n",
    "    with open(ndjson_file, 'r') as f:\n",
    "        for line in f:\n",
    "            builder.add_object(json.loads(line))\n",
    "\n",
    "    schema = builder.to_schema()\n",
    "    return schema\n",
    "\n",
    "def data_mapping_plan(files, destination_sample_file):\n",
    "    desired_schema = flatten_nested_dictionary(generate_schema(destination_sample_file))\n",
    "    sample = pd.read_json(destination_sample_file, lines=True)\n",
    "    df_normalized = sample.apply(lambda x: pd.json_normalize(x) if isinstance(x, dict) else x)  # type: Union[pd.DataFrame, Any]\n",
    "\n",
    "    loaded_source_files = {}\n",
    "    for file in files:\n",
    "        loaded_source_files[os.path.splitext(os.path.basename(file))[0]] = pd.read_csv(file)\n",
    "\n",
    "    destination_table = os.path.splitext(os.path.basename(destination_sample_file))[0]\n",
    "    \n",
    "    command = f\"\"\"\n",
    "    You are a professional data engineer and healthcare expert. Create a SQL transformation to transform records from the {loaded_source_files.keys()} into records for the {destination_table} table. Don't generate the entire SQL transformation yet.\n",
    "    \"\"\"\n",
    "    destination_prompt = f\"\"\"The {destination_table} table schema:\"\"\"\n",
    "    for name, value in desired_schema.items():\n",
    "        destination_prompt = destination_prompt + f\"- {name}:{value}\"\n",
    "    \n",
    "    destination_sample_prompt = f\"\"\"{destination_table} sample:\\n {str(df_normalized.head(5))}\"\"\"\n",
    "\n",
    "    source_prompts = []\n",
    "    for source_name, df in loaded_source_files.items():\n",
    "        source_prompts.append(f\"\"\"{source_name} sample:\\n {str(df.head(5))}\"\"\")\n",
    "\n",
    "    context_prompt = '\\n'.join([command, destination_prompt, destination_sample_prompt, *source_prompts])\n",
    "    unique_key = list(desired_schema.keys())[0]\n",
    "    template = context_prompt + \"\"\"\\n \n",
    "        {chat_history}\n",
    "        Generate the SQL query for id, {desired_column}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"desired_column\"], \n",
    "        template=template\n",
    "    )\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"desired_column\")\n",
    "    chain = load_qa_chain(OpenAI(temperature=0, openai_api_key=API), chain_type=\"stuff\", memory=memory, prompt=prompt)\n",
    "    return chain({\"input_documents\": desired_schema.keys(), \"human_input\": query}, return_only_outputs=True)\n",
    "    # prompt chain\n",
    "        # ask what field should be used to map two files together, foreign keys?\n",
    "        # provide destination records\n",
    "        # provide sample inputs\n",
    "\n",
    "dest_sample = 'data/Patient.ndjson'\n",
    "source_files=[\n",
    "    'data/mimic-iv-clinical-database-demo-2.2/hosp/admissions.csv',\n",
    "    'data/mimic-iv-clinical-database-demo-2.2/hosp/patients.csv',\n",
    "]\n",
    "plan = data_mapping_plan(\n",
    "    files=source_files,\n",
    "    destination_sample_file=dest_sample\n",
    ")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$schema': 'http://json-schema.org/schema#',\n",
       " 'type': 'object',\n",
       " 'properties': {'resourceType': {'type': 'string'},\n",
       "  'id': {'type': 'string'},\n",
       "  'meta': {'type': 'object',\n",
       "   'properties': {'versionId': {'type': 'string'},\n",
       "    'lastUpdated': {'type': 'string'},\n",
       "    'source': {'type': 'string'},\n",
       "    'profile': {'type': 'array', 'items': {'type': 'string'}}},\n",
       "   'required': ['lastUpdated', 'profile', 'source', 'versionId']},\n",
       "  'text': {'type': 'object',\n",
       "   'properties': {'status': {'type': 'string'}, 'div': {'type': 'string'}},\n",
       "   'required': ['div', 'status']},\n",
       "  'extension': {'type': 'array',\n",
       "   'items': {'type': 'object',\n",
       "    'properties': {'url': {'type': 'string'},\n",
       "     'extension': {'type': 'array',\n",
       "      'items': {'type': 'object',\n",
       "       'properties': {'url': {'type': 'string'},\n",
       "        'valueCoding': {'type': 'object',\n",
       "         'properties': {'system': {'type': 'string'},\n",
       "          'code': {'type': 'string'},\n",
       "          'display': {'type': 'string'}},\n",
       "         'required': ['code', 'display', 'system']},\n",
       "        'valueString': {'type': 'string'}},\n",
       "       'required': ['url']}},\n",
       "     'valueCode': {'type': 'string'}},\n",
       "    'required': ['url']}},\n",
       "  'identifier': {'type': 'array',\n",
       "   'items': {'type': 'object',\n",
       "    'properties': {'system': {'type': 'string'}, 'value': {'type': 'string'}},\n",
       "    'required': ['system', 'value']}},\n",
       "  'name': {'type': 'array',\n",
       "   'items': {'type': 'object',\n",
       "    'properties': {'use': {'type': 'string'}, 'family': {'type': 'string'}},\n",
       "    'required': ['family', 'use']}},\n",
       "  'gender': {'type': 'string'},\n",
       "  'birthDate': {'type': 'string'},\n",
       "  'deceasedDateTime': {'type': 'string'},\n",
       "  'maritalStatus': {'type': 'object',\n",
       "   'properties': {'coding': {'type': 'array',\n",
       "     'items': {'type': 'object',\n",
       "      'properties': {'system': {'type': 'string'}, 'code': {'type': 'string'}},\n",
       "      'required': ['code', 'system']}}},\n",
       "   'required': ['coding']},\n",
       "  'communication': {'type': 'array',\n",
       "   'items': {'type': 'object',\n",
       "    'properties': {'language': {'type': 'object',\n",
       "      'properties': {'coding': {'type': 'array',\n",
       "        'items': {'type': 'object',\n",
       "         'properties': {'system': {'type': 'string'},\n",
       "          'code': {'type': 'string'}},\n",
       "         'required': ['code', 'system']}}},\n",
       "      'required': ['coding']}},\n",
       "    'required': ['language']}},\n",
       "  'managingOrganization': {'type': 'object',\n",
       "   'properties': {'reference': {'type': 'string'}},\n",
       "   'required': ['reference']}},\n",
       " 'required': ['birthDate',\n",
       "  'extension',\n",
       "  'gender',\n",
       "  'id',\n",
       "  'identifier',\n",
       "  'managingOrganization',\n",
       "  'maritalStatus',\n",
       "  'meta',\n",
       "  'name',\n",
       "  'resourceType',\n",
       "  'text']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key1: value1\n",
      "key2.key3: value3\n",
      "key2.key4.key5: value5\n",
      "key6: value6\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackaithon-P5VjHMQO-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
